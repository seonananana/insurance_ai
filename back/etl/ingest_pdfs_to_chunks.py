# ETL í•œ ë°©ì— ë â€” PDF â†’ ì²­í¬ â†’ ìž„ë² ë”© â†’ DB (768ì°¨ì›)
import os, re, json
from pathlib import Path
from typing import List
import fitz  # PyMuPDF
from sqlalchemy import create_engine, text as sql, event
from dotenv import load_dotenv

# ðŸ”§ ìž„ë² ë”© í´ë¼ì´ì–¸íŠ¸
from app.services.embeddings_factory import get_embeddings_client
from sentence_transformers import SentenceTransformer

# .env ëª…ì‹œì ìœ¼ë¡œ ë¡œë“œ (REPL/ëª¨ë“ˆ ëª¨ë‘ ì•ˆì „)
ROOT = Path(__file__).resolve().parents[1]
load_dotenv(ROOT / ".env")

DATA_DIR = ROOT / "data"


# âŒ (ì‚­ì œ) ìž˜ëª»ëœ/ì¤‘ë³µ ì´ˆê¸°í™”
# EMB = get_embeddings_client(MODEL)

# --- DB ì—”ì§„ & pgvector ì–´ëŒ‘í„° ë“±ë¡ ---
# pip install pgvector (ì¤‘ìš”)
try:
    from pgvector.psycopg import register_vector
except ImportError:
    raise SystemExit("`pip install pgvector`ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

ENGINE = create_engine(os.environ["DATABASE_URL"], future=True)

@event.listens_for(ENGINE, "connect")
def _register_vector(dbapi_conn, conn_record):
    # psycopg3 ì—°ê²°ì— pgvector ì–´ëŒ‘í„° ë“±ë¡
    register_vector(dbapi_conn)

# --- ìž„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ (ê¸°ë³¸: e5-base-v2 = 768ì°¨ì›) ---
# âœ… get_embeddings_client ëŠ” model_nameì´ ì•„ë‹ˆë¼ model ë¡œ ë°›ìŠµë‹ˆë‹¤.
# --- ìž„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ (e5-base-v2 = 768ì°¨ì› ê°•ì œ) ---
MODEL_ID = os.getenv("EMBED_MODEL", "intfloat/e5-base-v2").split("#",1)[0].strip()
DEVICE   = os.getenv("EMBED_DEVICE", "cpu").split("#",1)[0].strip()

_model = SentenceTransformer(MODEL_ID, device=DEVICE)

class _EmbedWrapper:
    def embed(self, texts, is_query: bool):
        # e5 ê³„ì—´ ê¶Œìž¥ í”„ë¦¬í”½ìŠ¤
        if is_query:
            texts = [f"query: {t}" for t in texts]
        else:
            texts = [f"passage: {t}" for t in texts]
        # normalize_embeddings=True â†’ cosine/L2 ì¼ê´€ì„±
        return _model.encode(texts, normalize_embeddings=True).tolist()

EMB = _EmbedWrapper()


def extract_text(pdf_path: Path) -> str:
    with fitz.open(pdf_path) as doc:
        return "\n".join(page.get_text("text") for page in doc)

def simple_chunk(text: str, max_chars: int = 1400) -> List[str]:
    parts = re.split(r"(\n{2,}|[.!?])", text)
    chunks, buf = [], ""
    for p in parts:
        if len(buf) + len(p) > max_chars and buf:
            chunks.append(buf.strip()); buf = p
        else:
            buf += p
    if buf.strip():
        chunks.append(buf.strip())

    merged, acc = [], ""
    for c in chunks:
        if len(acc) + len(c) < max_chars // 2:
            acc += ("" if not acc else "\n") + c
        else:
            if acc: merged.append(acc); acc = ""
            merged.append(c)
    if acc:
        merged.append(acc)
    return [c for c in merged if len(c) >= 80]

def embed_passages(texts: List[str], batch_size: int = 64) -> List[List[float]]:
    out: List[List[float]] = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        vecs = EMB.embed(batch, is_query=False)  # íŒ¨ì‹œì§€ ìž„ë² ë”©
        if not vecs:
            raise RuntimeError("Embedding returned empty batch")
        out.extend(vecs)
    # ì°¨ì› í™•ì¸(768)
    if out and len(out[0]) != 768:
        raise ValueError(f"Embedding dim {len(out[0])} != 768 (DB schema)")
    return out

def main():
    pdfs = sorted(DATA_DIR.rglob("*.pdf"))
    if not pdfs:
        print(f"No PDFs under {DATA_DIR}")
        return

    with ENGINE.begin() as conn:
        for pdf in pdfs:
            policy_type = pdf.parent.name
            doc_id = pdf.stem
            print(f"[ingest] {pdf}")

            # documents í…Œì´ë¸” upsert(ì—†ìœ¼ë©´ ìƒì„±)
            conn.execute(sql("""
                INSERT INTO documents (doc_id, title, source_path, meta)
                VALUES (:doc_id, :title, :source_path, :meta)
                ON CONFLICT (doc_id) DO UPDATE
                SET title = EXCLUDED.title,
                    source_path = EXCLUDED.source_path
            """), {
                "doc_id": doc_id,
                "title": doc_id,
                "source_path": str(pdf),
                "meta": json.dumps({"policy_type": policy_type})
            })

            text = extract_text(pdf)
            if not text.strip():
                print("  -> empty text, skip")
                continue

            chunks = simple_chunk(text)
            if not chunks:
                print("  -> no chunks, skip")
                continue

            embs = embed_passages(chunks)

            # document_chunks ì»¬ëŸ¼ê³¼ ë§žì¶¤ (doc_id, chunk_index, content, embedding, meta)
            rows = []
            for i, (c, e) in enumerate(zip(chunks, embs)):
                rows.append({
                    "doc_id": doc_id,
                    "chunk_index": i,
                    "content": c,
                    "embedding": e,  # pgvector ì–´ëŒ‘í„°ê°€ list -> vector ë³€í™˜
                    "meta": json.dumps({
                        "policy_type": policy_type,
                        "clause_title": None
                    })
                })

            # âš ï¸ ê°™ì€ doc_idë¥¼ ìž¬ì ìž¬í•˜ë©´ ì¤‘ë³µë  ìˆ˜ ìžˆìŒ â†’ í•„ìš”í•˜ë©´ ì•„ëž˜ ì£¼ì„ í•´ì œí•´ ì„ ì‚­ì œ
            # conn.execute(sql("DELETE FROM document_chunks WHERE doc_id = :doc_id"), {"doc_id": doc_id})

            conn.execute(sql("""
                INSERT INTO document_chunks
                  (doc_id, chunk_index, content, embedding, meta)
                VALUES
                  (:doc_id, :chunk_index, :content, :embedding, :meta)
            """), rows)

            print(f"  -> chunks={len(chunks)}")

if __name__ == "__main__":
    main()
